Objective The objective of this module is to process a dataset of post-disaster street view images using a Multimodal Large Language Model (MLLM). The system is designed to perform two tasks: preliminary damage classification and the generation of structured image captions optimized for downstream CLIP model fine-tuning.

Dataset Organization The dataset is located at C:\Users\yyang295\Desktop\0310_post_folder. It is organized into three subfolders representing ground-truth damage severity:

folder_0: Mild Damage

folder_1: Moderate Damage

folder_2: Severe Damage

Processing Scope: The pipeline exclusively processes post-disaster images (e.g., files located within specific pair folders like ...\107806219091079_vs_3754051524846539(0)), disregarding pre-disaster baselines.

Processing Logic For each image, the model generates:

A Prediction: Categorizing the scene as mild, moderate, or severe.

A Description: A concise text caption focusing on visible disaster indicators (e.g., fallen trees, debris, infrastructure damage, flooding). The length and content of these descriptions are strictly controlled to ensure compatibility with CLIP training requirements.

